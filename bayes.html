<!DOCTYPE html>
<html>
    <head>
        <link href="https://fonts.googleapis.com/css?family=Schoolbell&v1" rel="stylesheet">
        <link rel="stylesheet" href="css/blog.css">
        <script type="text/javascript" src="js/jquery.min.js"></script>
        <script type="text/javascript" src="js/jslatex.js"></script>
        <script type="text/javascript">
            $(function () {
                $(".latex").latex({url: 'https://chart.apis.google.com/chart?cht=tx&chl={e}'});
            });
        </script>
        <title>Machine learning</title>
        <meta name="description" content="Mathematics for Machine Learning">
        <meta name="robots" content="index, follow" />
        <meta name="keywords" content="neural networks, svm, machine learning, linear algebra, bayes, PCA, SVG" />
    </head>
    <body>
        <div class="content">
            <a href="index.html" class="home">Home</a>
            <div class="blog-item">
                <div class="blog-header">Simple Bayes classifier</div>
                <div class="blog-shortly">
                    <div class="blog-paragraph">
                        Naïve Bayesian classifier is a simple approach to classify samples into multiple classes. 
                        Assume that we have vector <b>Y</b> of classes and vector of features <b>X</b>, we can then 
                        use the Bayes’ formula to find conditional probabilities as follows:
                    </div>
                    <div class="blog-paragraph">
                        <div class="latex">
                            P\left(y_j\middle|X\right)=\frac{P\left(y_j\right)}{P(x_1)...{P(x}_2)}\prod_{i=1}^{n}{P\left(x_i\middle| y_j\right)}
                        </div>
                    </div>
                    <div class="blog-paragraph">
                        Calculating <span class="latex">{P(y}_j)</span> and <span class="latex">{P(x}_i|y_j)</span> is trivial: 
                        the first probability can be calculated as a fraction of time 
                        class <span class="latex">y_j</span> appears in the dataset, and the second probability 
                        can be calculated as a fraction of time feature <span class="latex">x_i</span> 
                        appears in the class <span class="latex">y_j</span>. The above formula assumes that the features appear independently, hence the name of the 
                        classifier – naïve. The classification is therefore done as follows (we can ignore the denominator since it does 
                        not depend on class and is effectively a constant, i.e., it scales all results in a similar way):
                    </div>
                    <div class="blog-paragraph">
                        <div class="latex">
                            C=\ {argmax}_{\forall j\in k\ }P\left(y_j\right)\prod_{i=1}^{n}{P\left(x_i\middle| y_j\right)}
                        </div>
                    </div>

                    <div class="blog-paragraph">
                        To ease the computation, we can take the logarithm of the product to make it the sum. Moreover, 
                        for continuous data, the conditional probabilities can be assumed to come from normal distribution.
                        In this case, parameters <span class="latex">\mu</span> and <span class="latex">\sigma</span> are 
                        estimated from the empirical data, and the conditional probabilities are drawn from the Gaussian distribution
                        using the estimated parameters. 
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>