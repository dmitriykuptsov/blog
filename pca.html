<!DOCTYPE html>
<html>
    <head>
        <link href="https://fonts.googleapis.com/css?family=Schoolbell&v1" rel="stylesheet">
        <link rel="stylesheet" href="css/blog.css">
        <script type="text/javascript" src="js/jquery.min.js"></script>
        <script type="text/javascript" src="js/jslatex.js"></script>
        <script type="text/javascript">
            $(function () {
                $(".latex").latex({url: 'https://chart.apis.google.com/chart?cht=tx&chl={e}'});
            });
        </script>
        <title>Machine learning</title>
        <meta name="description" content="Mathematics for Machine Learning">
        <meta name="robots" content="index, follow" />
        <meta name="keywords" content="neural networks, svm, machine learning, linear algebra, bayes, PCA, SVD" />
    </head>
    <body>
        <div class="content">
            <a href="index.html" class="home">Home</a>
            <div class="blog-item">
                <div class="blog-header">Basics of Principal Component Analysis (PCA)</div>
                <div class="blog-shortly">
                    <div class="blog-paragraph">
                        When the number of features is too large, and some features do not introduce much 
                        information and so the dimension of the dataset may be reduced using Principal Component 
                        Analysis approach. Consider we are given a vector of observations, whereas each 
                        contains N features. In the PCA the first step is to subtract the mean from each 
                        feature as follows:
                    </div>
                    <div class="blog-paragraph">
                        <div class="latex">
                            Y=X-\bar{X}
                        </div>
                    </div>
                    <div class="blog-paragraph">
                        Now, all the datapoints are centered around zero, that is the mean of the features is zero. Consider the following case:
                    </div>
                    <div class="blog-paragraph" style="text-align: center">
                        <img src="img/PCA.png" width="50%" alt="Principal components"/>
                    </div>
                    <div class="blog-paragraph">
                        For simplicity, assume that we have 2-dimensional data. And consider that we have projected the datapoints on 
                        some vector <b>u</b>. The idea behind the PCA is to choose such vector <b>u</b> that maximizes the variance of 
                        the projections. Thus, we can write this fact mathematically as follows:

                        <div class="latex">
                            Var\ =\ \frac{1}{N}\sum{({ux}_i-u\bar{X}}){(ux_i-u\bar{X})}^T=u[\frac{1}{N}\sum{(y_i}){(y_i)}^T]u^T=uCu^T
                        </div>
                        

                        Observe that the term under the sum is just a covariance matrix of <b>Y</b>  
                        (it can be easily shown that it is  indeed so by expanding the multiplication of vector by itself).
                        Since the task is to maximize the variance and given that the magnitude of vector <b>u</b> should be bounded 
                        (otherwise the optimization will lead to infinitely large vector <b>u</b>) we say that <div class="latex">uu^T=1</div>
                        We can construct the Lagrangian and formulate the primal optimization problem (given the constraint):
                        <div class="latex">
                            L=uCu^T-\alpha(uu^T-1)
                        </div>
                        Taking the derivative of this expression with respect to vector <b>u</b> (matrix and vector derivatives are 
                        described <a href="https://en.wikipedia.org/wiki/Matrix_calculus">here</a>) and equating it to zero we get:
                        <div class="latex">
                            Cu^T=\alpha u^T
                        </div>
                        
                        But this is exactly the expression for the eigenvalues and eigenvectors. That is to maximize the variance we need 
                        to select the maximum value for the eigenvalue and select <b>u</b> to be corresponding eigenvector (this can be 
                        easily seen if we would plug the obtained equation into original one). At the end, we can construct the matrix in 
                        which rows are eigenvectors (ranked according to eigenvalues in the descending order), select top k rows and 
                        multiply the original vector <b>X</b> by the sought matrix of eigenvectors. This will be our reduced matrix:
                        <div class="latex">\hat{X}=XU^T</div>
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>