<html>
    <head>
        <link rel="stylesheet" href="css/blog.css">
    </head>
    <body>
        <div class="content">
            <div class="blog-title">
                Mathematics for machine learning
            </div>
            <div class="blog-menu">
                <a href="https://strangebit.io">Strangebit.io</a>
            </div>
            <div class="blog-item">
                <div class="blog-header">Introduction to linear algebra</div>
                <div class="blog-shortly">
                    Linear algebra is important in machine learning. It is in the heart
                    of many machine learning algorithms such as Singluar Value Decomposition (SVD) and
                    Principal Component Analysis (PCA). If you want to know how dimensionality reduction 
                    works you ought to understand what matrices and vectors are, what are eigenvalues and eigenvectors are,
                    what inverse of a matrix stand for, what is covariance matrix is, and many other 
                    important aspects. These are just few examples where linear algebra shines. 
                    
                    A very brief introduction (in just 4 pages) into linear algebra can be found
                    <a href="docs/linearAlgebra_4pgs.pdf">here</a>.
                </div>
            </div>
            <div class="blog-item">
                <div class="blog-header">Basics of Principal Component Analysis (PCA)</div>
                <div class="blog-shortly">
                    If you face a problem of large datasets and how to store those on disk
                    and use them to train the machine learning algorithms, dimensionality reduction
                    can help a lot. PCA allows one to reduce the amount of features in the dataset 
                    with minimal lose of information (well at the end user controls how much information
                    he or she is eager to lose during the transformation). 
                    <a href="pca.html">Read</a> more to discover how PCA works in practice.
                </div>
            </div>
            <div class="blog-item">
                <div class="blog-header">Approximating matrices with Singular Value Decomposition (SVD)</div>
                <div class="blog-shortly">
                    SVD can be viewed as generalization to PCA. Similarly to PCA, SVD allows compressing
                    the matrices with minimal loss of information (well, one can control how much compression he 
                    or she wants). In this article we show how to <a href="svd.html">decompose</a>
                    matrices using SVD.
                </div>
            </div>
            <div class="blog-item">
                <div class="blog-header">Large margin classifier</div>
                <div class="blog-shortly">
                    Understanding the mathematics behind algorithms is important for several reasons.
                    One such reason is that the reader will have more clear picture of what the 
                    algorithm is doing under the hood and how to fine tune it. In this article, we will discuss 
                    the mathematics behind <a href="svm.html">Support Vector Machine (SVM)</a>.
                </div>
            </div>
            <div class="blog-item">
                <div class="blog-header">Math behind Artificial Neural Networks (ANN)</div>
                Artificial neural networks (or ANN) gained a lot attention in the last several years. 
                Development of specialized hardware and availability of various libraries make 
                this type of supervised machine learning algorithms indispensable tools in solving
                challenging classification and regression problems. In this <a href="ANN.html">article</a> 
                we will describe the mathematics behind ANN using simple example.
                <div class="blog-shortly">
                </div>
            </div>
            <div class="blog-item">
                <div class="blog-header">Utilizing conditional probabilities in classification problems</div>
                <div class="blog-shortly">
                    Naive Bayes classifier is the simple approach to classify categorical and continuous 
                    data using conditional probabilities under the assumption that these probabilities 
                    are independent for different features in the dataset. In this <a href="bayes.html">article</a> we discuss 
                    how to use this type of classifier. 
                </div>
            </div>
            <!-- div class="blog-item">
                <div class="blog-header">Decision trees and random forests</div>
                <div class="blog-shortly">
                </div>
            </div -->
        </div>
    </body>
</html>