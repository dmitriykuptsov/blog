<!DOCTYPE html>
<html>
    <head>
        <link href="https://fonts.googleapis.com/css?family=Schoolbell&v1" rel="stylesheet">
        <link rel="stylesheet" href="css/blog.css">
        <script type="text/javascript" src="js/jquery.min.js"></script>
        <script type="text/javascript" src="js/jslatex.js"></script>
        <script type="text/javascript">
            $(function () {
                $(".latex").latex({url: 'https://chart.apis.google.com/chart?cht=tx&chl={e}'});
            });
        </script>
        <title>Machine learning</title>
        <meta name="description" content="Mathematics for Machine Learning">
        <meta name="robots" content="index, follow" />
        <meta name="keywords" content="neural networks, svm, machine learning, linear algebra, bayes, PCA, SVD" />
    </head>
    <body>
        <div class="content">
            <a href="index.html" class="home">Home</a>
            <div class="blog-item">
                <div class="blog-header">Mathematics behind Support Vector Machine</div>
                <div class="blog-shortly">
                    <div class="blog-paragraph">
                        Support vector machine is a powerful tool, and we believe that 
                        every data scientist needs to know the internals of this machine 
                        learning algorithm. We are going to discuss the idea behind this 
                        classification algorithm in the proceeding paragraphs.
                    </div>
                    <div class="blog-paragraph">
                        Given a hyperplane <b>wx</b>+<b>b</b>=<b>0</b> we can define two 
                        additional hyperplanes that are parallel to and equidistant from 
                        the given one. Namely, <b>wx</b>+<b>b</b>=<b>1</b> and 
                        <b>wx</b>+<b>b</b>=<b>-1</b>. If we say that we have a set of points 
                        <span class="latex">(x_i, y_i)</span> where <span class="latex">y_i</span> 
                        is either one or negative one (depending on the class to which the 
                        sample belongs), we state that the following must hold 
                        <span class="latex">y_i(wx+b)-1 \geq 0</span>, with equality occurring 
                        when points are exactly on the hyperplanes. We visualize the problem for the 2-D setting in the figure below:
                    </div>
                    <div class="blog-paragraph"  style="text-align: center">
                        <img src="img/SVM.png" width="50%" style="margin:auto" alt="Support vector machines hyperplane">
                    </div>
                    <div class="blog-paragraph">
                        Now if we take two datapoints, one on each hyperplane, parallel to and 
                        equidistant to <b>wx</b>+<b>b</b>=<b>0</b>, <span class="latex">x_1</span> and 
                        <span class="latex">x_2</span>, the margin between these two points can be 
                        expressed as follows (basically, the difference between the two vectors 
                        projected onto unit vector <b>w</b>, which is perpendicular to hyperplane 
                        that separates the points in different classes):
                    </div>
                    <div class="blog-paragraph">
                        <div class="latex">
                            \frac{(x_2-x_1)\cdot w}{||w||}=\ \frac{2}{||w||}
                        </div>
                    </div>

                    <div class="blog-paragraph">
                        According to SVM, the goal is to maximize this distance to reduce the classification 
                        error. However, maximizing this expression is the same as minimizing the following:
                    </div>
                    <div class="blog-paragraph">
                        
                        <div class="latex">
                            \frac{1}{2}{||w||}^2
                        </div>
                    </div>

                    <div class="blog-paragraph">
                        Since we are given constraints in a form yi(wx+b)-1â‰¥0 we can use Lagrange multipliers 
                        and construct the following equation:
                    </div>

                    <div class="blog-paragraph">
                        <div class="latex">
                            L=\frac{1}{2}w^Tw-\sum_{i=1}^{n}{\alpha_i (y_i (wx_i+b)-1)}
                        </div>
                    </div>

                    <div class="blog-paragraph">
                        To minimize this function, we need to find the partial derivatives with respect to 
                        vector w and b:
                    </div>
                    
                    <div class="blog-paragraph">
                        <div class="latex">
                            \frac{\partial L}{\partial w}=w-\sum_{i=1}^{n}{\alpha_iy_ix_i}\ =\ 0
                        </div>
                        <div class="latex">
                            \frac{\partial L}{\partial b}=\sum_{i=1}^{n}{\alpha_iy_i}=0
                        </div>
                    </div>

                    <div class="blog-paragraph">
                        If we now plug in the values for w into the original equation and 
                        reduce it (here we also use the result for the second partial 
                        derivative), we obtain (dual optimization problem):
                    </div>
                    
                    <div class="blog-paragraph">
                        <div class="latex">
                            L_D=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}{\alpha_i\alpha_jy_iy_jx_ix_j}
                        </div>
                        <div class="latex">
                            s.t. \sum_{i=1}^{n}{\alpha_iy_i=0}
                        </div>
                    </div>

                    <div class="blog-paragraph">
                        We can now solve this maximization problem using Quadratic Programming (QP) solver 
                        and obtain Lagrange multipliers. Once this is done, we can find the vector <b>w</b> 
                        and bias term <b>b</b>. For the last point, we can take a point 
                        <span class="latex">{(x_s,\ y}_s)\</span> on the hyperplane:
                    </div>
                    
                    <div class="blog-paragraph">
                        <div class="latex">
                            y_s(wx_s + b)=1\ \rightarrow\ b=\ y_s-wx_s=y_s-\sum_{i=1}^{n}{\alpha_iy_ix_i}x_s
                        </div>
                    </div>

                    <div class="blog-paragraph">
                        The derivations that we have presented are good only for linearly separable samples, 
                        however, one can use so called kernel trick to classify more complex datasets.
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>